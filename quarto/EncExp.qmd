--- 
title: "CompStats"
format: 
  dashboard:
    logo: images/ingeotec.png
    orientation: columns
    nav-buttons: [github]
    theme: cosmo
execute:
  freeze: auto    
---

# Introduction

## Column 

::: {.card title='Introduction' .flow}  
EncExp is a set of tools to create and use explainable embeddings.
:::

::: {.card title='Installing using pip' .flow} 
A more general approach to installing `EncExp` is through the use of the command pip, as illustrated in the following instruction.

```{sh} 
pip install EncExp
```
::: 

# Quick Start Guide 

## Column

::: {.card title="Datasets" .flow} 
```{python}
#| echo: true

from zipfile import ZipFile
from microtc.utils import tweet_iterator
from encexp.utils import Download, DialectID_URL
X, y = [], []
for filename in ['es-mx-sample.json.zip',
                 'es-ar-sample.json.zip']:
    _ = Download(f'{DialectID_URL}/{filename}',
                 filename)
    with ZipFile(filename, "r") as fpt:
        fpt.extractall(path=".",
                        pwd="ingeotec".encode("utf-8"))
    _ = list(tweet_iterator(filename.replace('.zip', '')))
    X.extend(_)
    y.extend([filename[3:5]] * len(_))
```
:::

::: {.card title="Training and Validation sets" .flow}
```{python}
#| echo: true

from sklearn.model_selection import train_test_split
Xtrain, Xval, ytrain, yval = train_test_split(X, y)
```
:::

::: {.card title="TextModel" .flow} 
```{python}
#| echo: true

from encexp import TextModel, SeqTM
from sklearn.svm import LinearSVC
from sklearn.pipeline import make_pipeline
tm = make_pipeline(TextModel(lang='es'),
                   LinearSVC()).fit(Xtrain, ytrain)
```
:::

## Column 

::: {.card title="TextModel (Corpus)" .flow}
```{python}
#| echo: true

corpus = make_pipeline(TextModel(lang='es', pretrained=False),
                       LinearSVC()).fit(Xtrain, ytrain)
```
:::

::: {.card title="SeqTM" .flow}
```{python}
#| echo: true

seq = make_pipeline(SeqTM(lang='es'),
                    LinearSVC()).fit(Xtrain, ytrain)
```
:::

::: {.card title="Performance" .flow}
```{python}
#| echo: true
from CompStats.metrics import macro_recall
score = macro_recall(yval, tm.predict(Xval),
                     name='TextModel')
_ = score(corpus.predict(Xval), name='TextModel (Corpus)')                     
_ = score(seq.predict(Xval), name='SeqTM')
score.plot()
```
:::

# Corpus 

## Column 

::: {.card title="Description"}
Tweets have been collected from the open stream for several years, e.g., the Spanish collection started on December 11, 2015 (see the table on the left to know the starting collection date for each language). The collected Tweets were filtered with the following restrictions: the retweets were removed; URL and users were replaced by the tokens _url and _usr, respectively; and only tweets with at least 50 characters were accepted in the final collection, namely Corpus. 

The Corpus is divided into two distinct sets: the first set is utilized to construct the training set, while the second set corresponds to the test set. The basis for this division is a specific date, with tweets published prior to October 1, 2022, forming the first set, and those published on October 3, 2022, or later, being used to create the test set. 

The training set and test set were created with an equivalent procedure; the only difference is that the maximum size of the training set is 10M tweets and $2^{12}$ (4096) tweets for the test set.

The training and test set was meticulously crafted by uniformly selecting the maximum number of tweets (i.e., 10M and $2^{12}$, respectively) from each day. These selected tweets were then organized by day, and within each day, the tweets were randomly chosen, with near duplicates being removed. The subsequent step involved the elimination of tweets that were near duplicates of the previous three days.

It is worth mentioning that the last step is to shuffle the training and test set to eliminate the ordering by date. 
:::

# Vocabulary 

## Column 
::: {.card title="Dataset" .flow}
```{python}
#| echo: true
from encexp.build_voc import compute_TextModel_vocabulary
from encexp.build_voc import compute_SeqTM_vocabulary
from microtc.utils import tweet_iterator

def iterator():
    files = ['es-mx-sample.json', 'es-ar-sample.json']
    for file in files:
        for x in tweet_iterator(file):
            yield x
```
:::

::: {.card title="TextModel" .flow}
```{python}
#| echo: true
params = compute_TextModel_vocabulary(iterator,
                                      pretrained=False,
                                      token_max_filter=2**19)
```                                             
:::

::: {.card title="SeqTM" .flow}
```{python}
#| echo: true

for voc in [18, 17]:
    params = compute_SeqTM_vocabulary(iterator,
                                      params,
                                      pretrained=False,
                                      token_max_filter=2**voc)
```                                             
:::

## Column 

::: {.card title="TextModel tokenizer" .flow}
```{python}
#| echo: true
tm = TextModel(lang='es')
tm.tokenize('¡buen día!')
```
:::

::: {.card title="SeqTM tokenizer" .flow}
```{python}
#| echo: true
seq = SeqTM(lang='es')
seq.tokenize('¡buen día!')
```
:::