--- 
title: "EncExp"
format: 
  dashboard:
    logo: images/ingeotec.png
    orientation: columns
    nav-buttons: [github]
    theme: cosmo
execute:
  freeze: auto
---

# Introduction

## Column 

::: {.card title='Introduction' .flow}  
EncExp is a set of tools for creating and using explainable embeddings. As with any embedding, the aim is to have a set of vectors that can be associated with tokens, and consequently, an utterance can be represented in the vector space span by the vectors. However, the difference concerning the embedding estimated with GloVe or Word2Vec, among others, is that EncExp associates vectors where each component has a meaning. The component's value indicates whether the word associated with the component might be present in the sentence. 

The component's meaning is a direct consequence of the procedure used to estimate the embedding. EncExp estimates the embedding by solving $d$ binary self-supervised classification problems, where the label is the presence of a particular token. The classifier used is a linear Support Vector Machine. 
:::

::: {.card title='Installing using pip' .flow} 
A more general approach to installing `EncExp` is through the use of the command pip, as illustrated in the following instruction.

```{sh} 
pip install EncExp
```
::: 

# Quick Start Guide 

## Column

::: {.card title="Datasets and libraries" .flow} 
```{python}
#| echo: true
#| code-fold: true

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import Normalizer
from sklearn.svm import LinearSVC
from sklearn.pipeline import make_pipeline
from CompStats.metrics import macro_recall
from encexp import TextModel, SeqTM, EncExpT
from encexp.utils import load_dataset

X, y = load_dataset(['mx', 'ar'], return_X_y=True)
Xtrain, Xval, ytrain, yval = train_test_split(X, y)
```
:::


::: {.card title="TextModel" .flow} 
```{python}
#| echo: true

tm = make_pipeline(TextModel(lang='es'),
                   LinearSVC()).fit(Xtrain, ytrain)
```
:::

::: {.card title="TextModel (Corpus)" .flow}
```{python}
#| echo: true

corpus = make_pipeline(TextModel(lang='es', pretrained=False),
                       LinearSVC()).fit(Xtrain, ytrain)
```
:::

::: {.card title="SeqTM" .flow}
```{python}
#| echo: true

seq = make_pipeline(SeqTM(lang='es'),
                    LinearSVC()).fit(Xtrain, ytrain)
```
:::

## Column 

::: {.card title="EncExp" .flow}
```{python}
#| echo: true

enc = make_pipeline(EncExpT(lang='es'),
                    Normalizer(),
                    LinearSVC()).fit(Xtrain, ytrain)
```
:::

::: {.card title="Performance" .flow}
```{python}
#| echo: true
#| code-fold: true

score = macro_recall(yval, tm.predict(Xval),
                     name='TextModel')
_ = score(corpus.predict(Xval), name='TextModel (Corpus)')                     
_ = score(seq.predict(Xval), name='SeqTM')
_ = score(enc.predict(Xval), name='EncExpT')
score.plot()
```
:::

# Corpora 

## Column {.tabset} 

```{python}
#| echo: false

from collections import defaultdict
from IPython.display import Markdown
import pandas as pd
from encexp.download import download_TextModel

def dataset_info(lang='es'):
    dataset = download_TextModel('dataset_info')
    # data = []
    # for line in dataset:
    #     if line['lang'] != lang:
    #         continue
    #     data = defaultdict(dict)

    #     cntr = data[]
    dataset = {data['set']: {k: v for k, v in data.items() 
                             if k not in ('lang', 'set')}
               for data in dataset if data['lang'] == lang}
    dataset = pd.DataFrame(dataset).reset_index(names='Country')
    return dataset.sort_values('Country')
```


::: {.card title='Arabic (ar)'}
```{python}
#| echo: false

Markdown(dataset_info('ar').to_markdown(index=False))
```
:::

::: {.card title='Catalan (ca)'}
```{python}
#| echo: false

Markdown(dataset_info('ca').to_markdown(index=False))
```
:::

::: {.card title='German (de)'}
```{python}
#| echo: false

Markdown(dataset_info('de').to_markdown(index=False))
```
:::

::: {.card title='English (en)'}
```{python}
#| echo: false

Markdown(dataset_info('en').to_markdown(index=False))
```
:::

::: {.card title='Spanish (es)'}
```{python}
#| echo: false

Markdown(dataset_info('es').to_markdown(index=False))
```
:::

::: {.card title='French (fr)'}
```{python}
#| echo: false

Markdown(dataset_info('fr').to_markdown(index=False))
```
:::

::: {.card title='Hindi (hi)'}
```{python}
#| echo: false

Markdown(dataset_info('hi').to_markdown(index=False))
```
:::

::: {.card title='Indonesian (in)'}
```{python}
#| echo: false

Markdown(dataset_info('in').to_markdown(index=False))
```
:::

::: {.card title='Italian (it)'}
```{python}
#| echo: false

Markdown(dataset_info('it').to_markdown(index=False))
```
:::

::: {.card title='Japanese (ja)'}
```{python}
#| echo: false

Markdown(dataset_info('ja').to_markdown(index=False))
```
:::

::: {.card title='Korean (ko)'}
```{python}
#| echo: false

Markdown(dataset_info('ko').to_markdown(index=False))
```
:::

::: {.card title='Dutch (nl)'}
```{python}
#| echo: false

Markdown(dataset_info('nl').to_markdown(index=False))
```
:::

::: {.card title='Polish (pl)'}
```{python}
#| echo: false

Markdown(dataset_info('pl').to_markdown(index=False))
```
:::

::: {.card title='Portuguese (pt)'}
```{python}
#| echo: false

Markdown(dataset_info('pt').to_markdown(index=False))
```
:::

::: {.card title='Russian (ru)'}
```{python}
#| echo: false

Markdown(dataset_info('ru').to_markdown(index=False))
```
:::

::: {.card title='Tagalog (tl)'}
```{python}
#| echo: false

Markdown(dataset_info('tl').to_markdown(index=False))
```
:::

::: {.card title='Turkish (tr)'}
```{python}
#| echo: false

Markdown(dataset_info('tr').to_markdown(index=False))
```
:::

::: {.card title='Chinese (zh)'}
```{python}
#| echo: false

Markdown(dataset_info('zh').to_markdown(index=False))
```
:::

## Column 

::: {.card title="Description"}
The dataset used to create the self-supervised problems is a collection of Tweets collected from the open stream for several years, i.e., the Spanish collection started on December 11, 2015; English on July 1, 2016; Arabic on January 25, 2017; Russian on October 16, 2018; and the rest of the languages on June 1, 2021. In all the cases, the last day collected was June 9, 2023. The collected Tweets were filtered with the following restrictions: retweets were removed; URLs and usernames were replaced by the tokens _url and _usr, respectively; and only tweets with at least 50 characters were included in the final collection. 

The corpora are divided into two sets: the first set is used as a training set, i.e., to estimate the parameters, while the second set corresponds to the test set, which could be used to measure the model's performance. The basis for this division is a specific date, with tweets published before October 1, 2022, forming the first set. Those published on or after October 3, 2022, are being used to create the test set. 

The training set and test set were created using an equivalent procedure; the only difference is that the maximum size of the training set is $2^{23}$ (8 million tweets), and the test set is $2^{12}$ (4,096 tweets).

There are pairs of training and test sets for each country, using tweets with geographic information, and a pair that groups all tweets without geographic information, labeled as ALL. Each set was meticulously crafted to have, as closely as possible, a uniform distribution of the days. Within each day, near duplicates were removed. Then, a three-day sliding window was used to remove near duplicates within the window. The final step was to shuffle the data to remove the ordering by date. 
:::

